{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Building a Simple Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will build a simple search index, which we will use later for Boolean retrieval. The assignment tasks are again at the bottom of this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summaries_file = 'data/allergy_Summaries.pkl.bz2'\n",
    "Abstracts_file = 'data/allergy_Abstracts.pkl.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, bz2\n",
    "from collections import namedtuple\n",
    "\n",
    "Summaries = pickle.load( bz2.BZ2File( Summaries_file, 'rb' ) )\n",
    "\n",
    "paper = namedtuple( 'paper', ['title', 'authors', 'year', 'doi'] )\n",
    "\n",
    "for (id, paper_info) in Summaries.items():\n",
    "    Summaries[id] = paper( *paper_info )\n",
    "    \n",
    "Abstracts = pickle.load( bz2.BZ2File( Abstracts_file, 'rb' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what the data looks like for an example of a paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paper(title='Type 2 innate lymphoid cells control eosinophil homeostasis.', authors=['Nussbaum JC', 'Van Dyken SJ', 'von Moltke J', 'Cheng LE', 'Mohapatra A', 'Molofsky AB', 'Thornton EE', 'Krummel MF', 'Chawla A', 'Liang HE', 'Locksley RM'], year=2013, doi='10.1038/nature12526')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Summaries[24037376]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eosinophils are specialized myeloid cells associated with allergy and helminth infections. Blood eosinophils demonstrate circadian cycling, as described over 80\\u2009years ago, and are abundant in the healthy gastrointestinal tract. Although a cytokine, interleukin (IL)-5, and chemokines such as eotaxins mediate eosinophil development and survival, and tissue recruitment, respectively, the processes underlying the basal regulation of these signals remain unknown. Here we show that serum IL-5 levels are maintained by long-lived type 2 innate lymphoid cells (ILC2) resident in peripheral tissues. ILC2 cells secrete IL-5 constitutively and are induced to co-express IL-13 during type 2 inflammation, resulting in localized eotaxin production and eosinophil accumulation. In the small intestine where eosinophils and eotaxin are constitutive, ILC2 cells co-express IL-5 and IL-13; this co-expression is enhanced after caloric intake. The circadian synchronizer vasoactive intestinal peptide also stimulates ILC2 cells through the VPAC2 receptor to release IL-5, linking eosinophil levels with metabolic cycling. Tissue ILC2 cells regulate basal eosinophilopoiesis and tissue eosinophil accumulation through constitutive and stimulated cytokine expression, and this dissociated regulation can be tuned by nutrient intake and central circadian rhythms. '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Abstracts[24037376]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define some utility functions that allow us to tokenize a string into terms, perform linguistic preprocessing on a list of terms, as well as a function to display information about a paper in a nice way. Note that these tokenization and preprocessing functions are rather naive. We will improve them in a later assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lorem', 'ipsum', 'dolor', 'sit', 'amet']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Function that tokenizes a string in a rather naive way. Can be extended later.\n",
    "    \"\"\"\n",
    "    return text.split(' ')\n",
    "\n",
    "def preprocess(tokens):\n",
    "    \"\"\"\n",
    "    Perform linguistic preprocessing on a list of tokens. Can be extended later.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        result.append(token.lower())\n",
    "    return result\n",
    "\n",
    "print(preprocess(tokenize(\"Lorem ipsum dolor sit AMET\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1038/nature12526>Type 2 innate lymphoid cells control eosinophil homeostasis.</a></strong><br>2013. Nussbaum JC, Van Dyken SJ, von Moltke J, Cheng LE, Mohapatra A, Molofsky AB, Thornton EE, Krummel MF, Chawla A, Liang HE, Locksley RM<br>[ID: 24037376]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1038/nature12526>Type 2 innate lymphoid cells control eosinophil homeostasis.</a></strong><br>2013. Nussbaum JC, Van Dyken SJ, von Moltke J, Cheng LE, Mohapatra A, Molofsky AB, Thornton EE, Krummel MF, Chawla A, Liang HE, Locksley RM<br><small><strong>Abstract:</strong> <em>Eosinophils are specialized myeloid cells associated with allergy and helminth infections. Blood eosinophils demonstrate circadian cycling, as described over 80â€‰years ago, and are abundant in the healthy gastrointestinal tract. Although a cytokine, interleukin (IL)-5, and chemokines such as eotaxins mediate eosinophil development and survival, and tissue recruitment, respectively, the processes underlying the basal regulation of these signals remain unknown. Here we show that serum IL-5 levels are maintained by long-lived type 2 innate lymphoid cells (ILC2) resident in peripheral tissues. ILC2 cells secrete IL-5 constitutively and are induced to co-express IL-13 during type 2 inflammation, resulting in localized eotaxin production and eosinophil accumulation. In the small intestine where eosinophils and eotaxin are constitutive, ILC2 cells co-express IL-5 and IL-13; this co-expression is enhanced after caloric intake. The circadian synchronizer vasoactive intestinal peptide also stimulates ILC2 cells through the VPAC2 receptor to release IL-5, linking eosinophil levels with metabolic cycling. Tissue ILC2 cells regulate basal eosinophilopoiesis and tissue eosinophil accumulation through constitutive and stimulated cytokine expression, and this dissociated regulation can be tuned by nutrient intake and central circadian rhythms. </em></small><br>[ID: 24037376]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import re\n",
    "\n",
    "def display_summary( id, show_abstract=False, show_id=True, extra_text='' ):\n",
    "    \"\"\"\n",
    "    Function for printing a paper's summary through IPython's Rich Display System.\n",
    "    Trims long author lists, and adds a link to the paper's DOI (when available).\n",
    "    \"\"\"\n",
    "    s = Summaries[id]\n",
    "    lines = []\n",
    "    title = s.title\n",
    "    if s.doi != '':\n",
    "        title = '<a href=http://dx.doi.org/{:s}>{:s}</a>'.format(s.doi, title)\n",
    "    title = '<strong>' + title + '</strong>'\n",
    "    lines.append(title)\n",
    "    authors = ', '.join( s.authors[:20] ) + ('' if len(s.authors) <= 20 else ', ...')\n",
    "    lines.append(str(s.year) + '. ' + authors)\n",
    "    if (show_abstract):\n",
    "        lines.append('<small><strong>Abstract:</strong> <em>{:s}</em></small>'.format(Abstracts[id]))\n",
    "    if (show_id):\n",
    "        lines.append('[ID: {:d}]'.format(id))\n",
    "    if (extra_text != ''):\n",
    "         lines.append(extra_text)\n",
    "    display( HTML('<br>'.join(lines)) )\n",
    "\n",
    "display_summary(24037376)\n",
    "display_summary(24037376, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our first index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create an _inverted index_ based on the words in the abstracts of the papers in our dataset.\n",
    "\n",
    "We will implement our inverted index as a **Python dictionary with terms as keys and posting lists as values**. For the posting lists, instead of using Python lists and then implementing the different operations on them ourselves, we will use **Python sets** and use the predefined set operations to process these posting \"lists\". This will also ensure that each document is added at most once per term. The use of Python sets is not the most efficient solution but will work for our purposes. (As an optional additional exercise, you can try to implement the posting lists as Python lists for this and the following assignments.)\n",
    "\n",
    "Not every paper in our dataset has an abstract; we will only index papers for which an abstract is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "inverted_index = defaultdict(set)\n",
    "\n",
    "# This may take a while:\n",
    "for (id, abstract) in Abstracts.items():\n",
    "    for term in preprocess(tokenize(abstract)):\n",
    "        inverted_index[term].add(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's in the index for the example term 'amsterdam':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{16513592, 18706961, 15504455}\n"
     ]
    }
   ],
   "source": [
    "print(inverted_index['amsterdam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this inverted index to answer simple one-word queries, for example to show all papers that contain the word 'rotterdam':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1046/j.1442-2042.10.s1.10.x>Systemic aspects of interstitial cystitis, immunology and linkage with autoimmune disorders.</a></strong><br>2003. van de Merwe JP, Yamada T, Sakamoto Y<br>[ID: 14641413]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<strong><a href=http://dx.doi.org/10.1093/oxfordjournals.aje.a117332>Toxocara seroprevalence in 5-year-old elementary schoolchildren: relation with allergic asthma.</a></strong><br>1994. Buijs J, Borsboom G, van Gemund JJ, Hazebroek A, van Dongen PA, van Knapen F, Neijens HJ<br>[ID: 7977294]"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_word = 'rotterdam'\n",
    "for i in inverted_index[query_word]:\n",
    "    display_summary(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your name:** Polina Boneva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Construct a function called `and_query` that takes as input a single string, consisting of one or more words, such that the function returns a list of matching documents. `and_query`, as its name suggests, should require that all query terms are present in the documents of the result list. Demonstrate the working of your function with an example (choose one that leads to fewer than 100 hits to not overblow this notebook file).\n",
    "\n",
    "(You can use the `tokenize` and `preprocess` functions we defined above to tokenize and preprocess your query. You can also exploit the fact that the posting lists are [sets](https://docs.python.org/3/library/stdtypes.html#set), which means you can easily perform set operations such as union, difference and intersect on them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{16513592, 18706961, 15504455}\n",
      "{24037376, 21391459, 12967780, 8299387, 22767057, 26259611, 20736733}\n",
      "{22767057}\n",
      "{10477723, 28798252, 30680604}\n"
     ]
    }
   ],
   "source": [
    "def and_query(stringToMatch):\n",
    "    themWords = preprocess(tokenize(stringToMatch))\n",
    "    fullSetOfDocs = [inverted_index[word] for word in themWords]\n",
    "    if (fullSetOfDocs):\n",
    "        return set.intersection(*fullSetOfDocs)\n",
    "    else:\n",
    "        return 'No documents contain this query in full'\n",
    "\n",
    "print(and_query('amsterdam'))\n",
    "print(and_query('resident in peripheral'))\n",
    "print(and_query('resident in peripheral tissues'))\n",
    "print(and_query('eosinophil signal inflammation'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Construct a second function called `or_query` that works in the same way as `and_query` you just implemented, but returns as function value the documents that contain _at least one_ of the words in the query. Demonstrate the working of this second function also with an example (again, choose one that leads to fewer than 100 hits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{70944, 21950132, 8317015, 852905, 11876122, 12045423}\n"
     ]
    }
   ],
   "source": [
    "def or_query(stringToMatch):\n",
    "    themWords = preprocess(tokenize(stringToMatch))\n",
    "    fullSetOfDocs = [inverted_index[word] for word in themWords]\n",
    "    if (fullSetOfDocs):\n",
    "        return fullSetOfDocs[0].union(*fullSetOfDocs)\n",
    "    else:\n",
    "        return 'No documents contain this query even partially'\n",
    "    \n",
    "print(or_query('superfluous'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Show how many hits the query \"to be or not to be\" returns for your two query functions (`and_query` and `or_query`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3536\n",
      "44826\n"
     ]
    }
   ],
   "source": [
    "print(len(and_query('to be or not to be')))\n",
    "print(len(or_query('to be or not to be')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Given the nature of our dataset, how many of the documents from task 3 do you think are actually about the [Shakespeare quote](https://en.wikipedia.org/wiki/To_be%2C_or_not_to_be)? What could you do to better handle such queries? (You don't have to implement this yet!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** None of them are probably about the Shakespeare quote, but a lot of them are a match because those words are used in almost every meaningful paragraph to connect other words, so that sentences are developed with junctions and not only nouns and verbs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "Why does `and_query('eosinophil signal inflammation')` not return our example paper 24037376, even though it mentions eosinophil, signal, and inflammation in the abstract? (You do not have to implement anything to fix this yet!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** It just so happens that our tokenization function does not do more than simply break the query string by empty space. Thus, what is actually found in the paper are the words 'signals' and 'inflammation,' - stemming would help here, if we were looking for 'signal' and 'inflammation' as parts of the word to be found in each document, but since we are using them as exact matches, signals and inflammation with a comma do not get included in the same document lists as signal and inflammation without a comma. Paper 24037376 is really on the list of 'eosinophil' only, out of the given three-word query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the answers to the assignment via Canvas as a modified version of this Notebook file (file with `.ipynb` extension) that includes your code and your answers.\n",
    "\n",
    "Before submitting, restart the kernel and re-run the complete code (**Kernel > Restart & Run All**), and then check whether your assignment code still works as expected.\n",
    "\n",
    "Don't forget to add your name, and remember that the assignments have to be done individually and group submissions are **not allowed**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
